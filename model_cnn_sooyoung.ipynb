{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3189,
     "status": "ok",
     "timestamp": 1601048600688,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "aIvnImlO7m1s",
    "outputId": "d8aae6e3-5791-4a66-ff74-362d7ecba2c2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1601049242892,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "X7E2Jsjh7m1x"
   },
   "outputs": [],
   "source": [
    "# labelNames = ['nsleeve', 'ssleeve', 'lsleeve', 'jsuit', 'collar', 'hood', 'coat', 'jacket', 'lpadding', 'spadding', 'cardigan', 'vest', 'spants', 'lpants', 'skirt']\n",
    "# labelNames = ['0_0', '0_1', '0_2', '0_3_l', '0_3_s', '0_4_l', '0_4_s', '0_5', '0_6', '0_7',\n",
    "#              '1_0', '1_1', '1_2', '1_3', '1_4', '1_5', '1_6', '1_7', '1_8', '1_9', '1_10', '1_11', '1_12', '1_13', '1_14', '1_15', '1_16', '1_17',\n",
    "#              '2_0', '2_1', '2_2', '2_3',\n",
    "#              '3_0', '3_1', '3_2', '3_3', '3_4', '3_5',\n",
    "#              '4_0', '4_1', '4_2']\n",
    "# classMatching = [([0, 11], [1], [2, 5, 6, 7, 8, 9, 10], [12, 13], [14], [3], [4])]\n",
    "# classDic = { 0:1, 1:2, 2:0, 3:2, 4:1, 5:2, 6:1, 7:2, 8:2, 9:2,\n",
    "#            10:2, 11:2, 12:2, 13:2, 14:2, 15:2, 16:2, 17:2, 18:2, 19:2, 20:2, 21:2, 22:2, 23:2, 24:2, 25:0, 26:0, 27:2,\n",
    "#            28:5, 29:5, 30:5, 31:5,\n",
    "#            32:3, 33:3, 34:3, 35:3, 36:3, 37:3,\n",
    "#            38:4, 39:4, 40:4}\n",
    "\n",
    "# classNames = ['nsleeve', 'ssleeve', 'lsleeve', 'pants', 'skirt', 'dress']\n",
    "\n",
    "# classNames = ['nsleeve', 'vest']\n",
    "\n",
    "# classNames = ['ssleeve', 'collar']\n",
    "\n",
    "# classNames = ['sleeve', 'haveCollar', 'padding']\n",
    "# classNames = ['lsleeve', 'hood', 'cardigan']\n",
    "# classNames = ['coat', 'jacket', 'collar']\n",
    "# classNames = ['lpadding', 'spadding']\n",
    "\n",
    "# classNames = ['lpants', 'spants']\n",
    "\n",
    "# classNames = ['dress', 'jsuit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 numpy array로 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(data, per):\n",
    "    np.random.seed(123)\n",
    "    N = len(data)\n",
    "    sample_n = int(len(data)*per)\n",
    "    sample = data.take(np.random.permutation(N)[:sample_n])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64633,
     "status": "ok",
     "timestamp": 1601049309058,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "dAxzAV-Q7m10",
    "outputId": "99a037ab-c539-4894-d8ca-ab12f73cace2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_0  파일 길이 :  450\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "3_1  파일 길이 :  450\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "3_2  파일 길이 :  450\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "3_3  파일 길이 :  450\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "3_4  파일 길이 :  450\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "3_5  파일 길이 :  450\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "(2562, 125, 125)\n",
      "(2562, 2)\n",
      "(138, 125, 125)\n",
      "(138, 2)\n",
      "ok 2562 138\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "fileNames = []\n",
    "\n",
    "for idx, cat in enumerate(labelNames):\n",
    "\n",
    "    label = [0 for i in range(len(classNames))]\n",
    "    label[classDic[idx]] = 1\n",
    "\n",
    "    imgDir = dir + \"/\" + cat\n",
    "    files = glob.glob(imgDir+\"/*.png\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    \n",
    "    i = 0\n",
    "    lst = []\n",
    "    for i in range(len(files)) :\n",
    "        lst.append(i)\n",
    "    lst = pd.DataFrame(lst, columns = ['idx'])\n",
    "\n",
    "    trainSet = lst.apply(sampling, per=0.95)\n",
    "    trainSet = trainSet.sort_index()\n",
    "\n",
    "    testSet = lst.drop(lst.index[trainSet.index])\n",
    "    testSet = testSet.sort_index()\n",
    "\n",
    "    trainSet = trainSet['idx'].values.tolist()\n",
    "    testSet = testSet['idx'].values.tolist()\n",
    "    \n",
    "    for f in range(len(trainSet)):\n",
    "        img = cv2.imread(files[trainSet[f]])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        if f == 0:\n",
    "            print(label)\n",
    "            \n",
    "        x_train.append(data)\n",
    "        y_train.append(label)\n",
    "    \n",
    "    for f in range(len(testSet)):\n",
    "        img = cv2.imread(files[trainSet[f]])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        if f == 0:\n",
    "            print(label)\n",
    "            \n",
    "        x_test.append(data)\n",
    "        y_test.append(label)\n",
    "        fileNames.append(files[testSet[f]])\n",
    "\n",
    "# for idx in range(16) :\n",
    "#     imgDir = dir + '/' + labelNames[idx]\n",
    "#     files = glob.glob(imgDir+\"/*.png\")\n",
    "#     print(\"파일 길이 : \", len(files))\n",
    "    \n",
    "#     for f in files:\n",
    "#         img = Image.open(f)\n",
    "#         img = img.convert(\"RGB\")\n",
    "#         data = np.asarray(img, np.int32)\n",
    "        \n",
    "#         label = np.zeros(16)\n",
    "#         label[idx] = 1\n",
    "        \n",
    "#         x.append(data)\n",
    "#         y.append(label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(\"ok\", len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"imageData\", x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 859,
     "status": "ok",
     "timestamp": 1601049350619,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "C2zpryl67m12",
    "outputId": "a687406b-5ef9-4613-ce97-d4de5cbb9add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2562, 125, 125, 3)\n",
      "(138, 125, 125, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2, os, glob, numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Dense, Flatten, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.config.experimental\n",
    "\n",
    "imageLoad = np.load(\"data.npz\")\n",
    "x_train = imageLoad['x_train']\n",
    "y_train = imageLoad['y_train']\n",
    "x_test = imageLoad['x_test']\n",
    "y_test = imageLoad['y_test']\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1601049564198,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "Kla7-Rb-7m15"
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "x_train = x_train.astype(float) / 255\n",
    "x_train = np.reshape(x_train, (-1, 125, 125, 1))\n",
    "\n",
    "x_test = x_test.astype(float) / 255\n",
    "x_test = np.reshape(x_test, (-1, 125, 125, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2305, 125, 125, 3)\n",
      "(257, 125, 125, 3)\n",
      "(138, 125, 125, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계 : CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2305, 125, 125, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data = ImageDataGenerator(\n",
    "    # rotation_range = 30,\n",
    "    width_shift_range = 0.2, \n",
    "    height_shift_range = 0.2, \n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True)\n",
    "\n",
    "data.fit(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1601052925682,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "5MUtohyz7m18"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=x_train.shape[1:], activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.add(Reshape((2, 2)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_dir = './model'\n",
    "    \n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1601052932510,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "X13t5r177m1_",
    "outputId": "c79dc477-96d0-49c2-a71a-98c79dbffe63",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 125, 125, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 62, 62, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 3, 3, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 2, 2)              0         \n",
      "=================================================================\n",
      "Total params: 320,484\n",
      "Trainable params: 320,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계 : InceptionResnetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "\n",
    "base_model = InceptionResNetV2(include_top=False, pooling='avg')\n",
    "outputs = Dense(2, activation='softmax')(base_model.output)\n",
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-608bd824d1e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mteacher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mteacher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstudent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstudent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "teacher = Model(base_model.inputs, outputs)\n",
    "teacher.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "student = Model(base_model.inputs, outputs)\n",
    "student.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계 : InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG19,ResNet50,InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size must be at least 139x139; got `input_shape=(125, 125, 3)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-45d1cdd91076>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbase_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of layers in the base model: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_applications\\inception_v3.py\u001b[0m in \u001b[0;36mInceptionV3\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;31m# Determine proper input shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     input_shape = _obtain_input_shape(\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mdefault_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m299\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_applications\\imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[1;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[0;32m    311\u001b[0m                 if ((input_shape[0] is not None and input_shape[0] < min_size) or\n\u001b[0;32m    312\u001b[0m                    (input_shape[1] is not None and input_shape[1] < min_size)):\n\u001b[1;32m--> 313\u001b[1;33m                     raise ValueError('Input size must be at least ' +\n\u001b[0m\u001b[0;32m    314\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'x'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                                      \u001b[1;34m'; got `input_shape='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input size must be at least 139x139; got `input_shape=(125, 125, 3)`"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(include_top=False,weights='imagenet',input_shape=(125, 125, 3), classes=y_train.shape[1:])\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine tune from this layer onwards\n",
    "fine_tune_at = 30\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone = Sequential()\n",
    "clone.add(Conv2D(16, (3,3), padding=\"same\", input_shape=x_train.shape[1:], activation='relu'))\n",
    "clone.add(MaxPooling2D(pool_size=(2,2)))\n",
    "clone.add(Conv2D(16, (3,3), padding=\"same\", activation='relu'))\n",
    "clone.add(MaxPooling2D(pool_size=(2,2)))\n",
    "clone.add(Dropout(0.25))\n",
    "\n",
    "clone.add(Flatten())\n",
    "clone.add(Dense(32, activation='relu'))\n",
    "clone.add(Dropout(0.5))\n",
    "clone.add(Dense(4, activation='softmax'))\n",
    "clone.add(Reshape((2, 2)))\n",
    "\n",
    "clone.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_dir = './model'\n",
    "    \n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, None, 3) for input Tensor(\"input_5:0\", shape=(None, None, None, 3), dtype=float32), but it was called on an input with incompatible shape (None, 125, 125, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\n        return self._run_internal_graph(\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer conv2d_451 is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape [None, 125, 125, 1]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-29030259ce64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Train and evaluate teacher on data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\n        return self._run_internal_graph(\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer conv2d_451 is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape [None, 125, 125, 1]\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate teacher on data.\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "73/73 [==============================] - 51s 705ms/step - sparse_categorical_accuracy: 0.8364 - student_loss: 0.5537 - distillation_loss: 2.9274e-04\n",
      "Epoch 2/3\n",
      "73/73 [==============================] - 62s 855ms/step - sparse_categorical_accuracy: 0.8364 - student_loss: 0.5537 - distillation_loss: 2.9274e-04\n",
      "Epoch 3/3\n",
      "73/73 [==============================] - 53s 727ms/step - sparse_categorical_accuracy: 0.8364 - student_loss: 0.5537 - distillation_loss: 2.9317e-04\n",
      "5/5 [==============================] - 1s 115ms/step - sparse_categorical_accuracy: 0.8333 - student_loss: 0.5340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8333333134651184"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distiller = Distiller(student=model, teacher=clone)\n",
    "\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "distiller.save_weights('model_distill_pants.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train student as doen usually\n",
    "student_scratch.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate student trained from scratch.\n",
    "student_scratch.fit(x_train, y_train, epochs=3)\n",
    "student_scratch.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 140805,
     "status": "ok",
     "timestamp": 1601053428630,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "IF19f_if7m2C",
    "outputId": "e0f97556-a145-4a45-f5cd-436856413bb3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.8487 - accuracy: 0.6700\n",
      "Epoch 00001: val_loss improved from inf to 0.78872, saving model to ./model\\multi_img_classification.model\n",
      "WARNING:tensorflow:From C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 569ms/step - loss: 0.8487 - accuracy: 0.6700 - val_loss: 0.7887 - val_accuracy: 0.7027\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.8172 - accuracy: 0.6730\n",
      "Epoch 00002: val_loss improved from 0.78872 to 0.74111, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 91s 573ms/step - loss: 0.8172 - accuracy: 0.6730 - val_loss: 0.7411 - val_accuracy: 0.7027\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.8093 - accuracy: 0.6730\n",
      "Epoch 00003: val_loss did not improve from 0.74111\n",
      "159/159 [==============================] - 87s 545ms/step - loss: 0.8093 - accuracy: 0.6730 - val_loss: 0.7497 - val_accuracy: 0.7027\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.8067 - accuracy: 0.6730\n",
      "Epoch 00004: val_loss did not improve from 0.74111\n",
      "159/159 [==============================] - 88s 551ms/step - loss: 0.8067 - accuracy: 0.6730 - val_loss: 0.7513 - val_accuracy: 0.7027\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.7926 - accuracy: 0.6734\n",
      "Epoch 00005: val_loss did not improve from 0.74111\n",
      "159/159 [==============================] - 87s 545ms/step - loss: 0.7926 - accuracy: 0.6734 - val_loss: 0.7643 - val_accuracy: 0.7027\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.7892 - accuracy: 0.6751\n",
      "Epoch 00006: val_loss improved from 0.74111 to 0.68792, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 568ms/step - loss: 0.7892 - accuracy: 0.6751 - val_loss: 0.6879 - val_accuracy: 0.7381\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.7387 - accuracy: 0.7035\n",
      "Epoch 00007: val_loss improved from 0.68792 to 0.64589, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 564ms/step - loss: 0.7387 - accuracy: 0.7035 - val_loss: 0.6459 - val_accuracy: 0.7540\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.7068 - accuracy: 0.7133\n",
      "Epoch 00008: val_loss improved from 0.64589 to 0.64160, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 563ms/step - loss: 0.7068 - accuracy: 0.7133 - val_loss: 0.6416 - val_accuracy: 0.7345\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.6825 - accuracy: 0.7295\n",
      "Epoch 00009: val_loss improved from 0.64160 to 0.59637, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 91s 570ms/step - loss: 0.6825 - accuracy: 0.7295 - val_loss: 0.5964 - val_accuracy: 0.7681\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.6683 - accuracy: 0.7344\n",
      "Epoch 00010: val_loss did not improve from 0.59637\n",
      "159/159 [==============================] - 88s 556ms/step - loss: 0.6683 - accuracy: 0.7344 - val_loss: 0.6079 - val_accuracy: 0.7699\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.6485 - accuracy: 0.7411\n",
      "Epoch 00011: val_loss improved from 0.59637 to 0.56461, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 95s 594ms/step - loss: 0.6485 - accuracy: 0.7411 - val_loss: 0.5646 - val_accuracy: 0.7770\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.6385 - accuracy: 0.7468\n",
      "Epoch 00012: val_loss did not improve from 0.56461\n",
      "159/159 [==============================] - 87s 546ms/step - loss: 0.6385 - accuracy: 0.7468 - val_loss: 0.5872 - val_accuracy: 0.7805\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.6350 - accuracy: 0.7488\n",
      "Epoch 00013: val_loss did not improve from 0.56461\n",
      "159/159 [==============================] - 87s 549ms/step - loss: 0.6350 - accuracy: 0.7488 - val_loss: 0.5673 - val_accuracy: 0.7788\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.6127 - accuracy: 0.7592\n",
      "Epoch 00014: val_loss did not improve from 0.56461\n",
      "159/159 [==============================] - 87s 547ms/step - loss: 0.6127 - accuracy: 0.7592 - val_loss: 0.5720 - val_accuracy: 0.7912\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.7527\n",
      "Epoch 00015: val_loss improved from 0.56461 to 0.54573, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 567ms/step - loss: 0.6293 - accuracy: 0.7527 - val_loss: 0.5457 - val_accuracy: 0.7947\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.7592\n",
      "Epoch 00016: val_loss did not improve from 0.54573\n",
      "159/159 [==============================] - 86s 543ms/step - loss: 0.6083 - accuracy: 0.7592 - val_loss: 0.5545 - val_accuracy: 0.7823\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5890 - accuracy: 0.7637\n",
      "Epoch 00017: val_loss improved from 0.54573 to 0.52870, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 567ms/step - loss: 0.5890 - accuracy: 0.7637 - val_loss: 0.5287 - val_accuracy: 0.7876\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5955 - accuracy: 0.7624\n",
      "Epoch 00018: val_loss improved from 0.52870 to 0.52380, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 91s 572ms/step - loss: 0.5955 - accuracy: 0.7624 - val_loss: 0.5238 - val_accuracy: 0.7982\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.7669\n",
      "Epoch 00019: val_loss did not improve from 0.52380\n",
      "159/159 [==============================] - 88s 552ms/step - loss: 0.5756 - accuracy: 0.7669 - val_loss: 0.5316 - val_accuracy: 0.7929\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5658 - accuracy: 0.7771\n",
      "Epoch 00020: val_loss did not improve from 0.52380\n",
      "159/159 [==============================] - 87s 547ms/step - loss: 0.5658 - accuracy: 0.7771 - val_loss: 0.5504 - val_accuracy: 0.7876\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.7708\n",
      "Epoch 00021: val_loss improved from 0.52380 to 0.51963, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 568ms/step - loss: 0.5748 - accuracy: 0.7708 - val_loss: 0.5196 - val_accuracy: 0.8088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5623 - accuracy: 0.7805\n",
      "Epoch 00022: val_loss improved from 0.51963 to 0.51728, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 91s 572ms/step - loss: 0.5623 - accuracy: 0.7805 - val_loss: 0.5173 - val_accuracy: 0.7965\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.7805\n",
      "Epoch 00023: val_loss did not improve from 0.51728\n",
      "159/159 [==============================] - 86s 543ms/step - loss: 0.5496 - accuracy: 0.7805 - val_loss: 0.5267 - val_accuracy: 0.8071\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.7891\n",
      "Epoch 00024: val_loss improved from 0.51728 to 0.50601, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 564ms/step - loss: 0.5436 - accuracy: 0.7891 - val_loss: 0.5060 - val_accuracy: 0.8106\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.7870\n",
      "Epoch 00025: val_loss improved from 0.50601 to 0.48659, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 563ms/step - loss: 0.5447 - accuracy: 0.7870 - val_loss: 0.4866 - val_accuracy: 0.8230\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.7891\n",
      "Epoch 00026: val_loss did not improve from 0.48659\n",
      "159/159 [==============================] - 87s 545ms/step - loss: 0.5327 - accuracy: 0.7891 - val_loss: 0.4993 - val_accuracy: 0.8088\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.7846\n",
      "Epoch 00027: val_loss did not improve from 0.48659\n",
      "159/159 [==============================] - 87s 550ms/step - loss: 0.5389 - accuracy: 0.7846 - val_loss: 0.5146 - val_accuracy: 0.7876\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.7907\n",
      "Epoch 00028: val_loss did not improve from 0.48659\n",
      "159/159 [==============================] - 86s 543ms/step - loss: 0.5324 - accuracy: 0.7907 - val_loss: 0.4974 - val_accuracy: 0.8195\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 0.7974\n",
      "Epoch 00029: val_loss improved from 0.48659 to 0.47437, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 91s 570ms/step - loss: 0.5159 - accuracy: 0.7974 - val_loss: 0.4744 - val_accuracy: 0.8195\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.7909\n",
      "Epoch 00030: val_loss did not improve from 0.47437\n",
      "159/159 [==============================] - 86s 541ms/step - loss: 0.5247 - accuracy: 0.7909 - val_loss: 0.5397 - val_accuracy: 0.7876\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5236 - accuracy: 0.7929\n",
      "Epoch 00031: val_loss improved from 0.47437 to 0.47092, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 90s 566ms/step - loss: 0.5236 - accuracy: 0.7929 - val_loss: 0.4709 - val_accuracy: 0.8177\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.7992\n",
      "Epoch 00032: val_loss did not improve from 0.47092\n",
      "159/159 [==============================] - 86s 543ms/step - loss: 0.5144 - accuracy: 0.7992 - val_loss: 0.4832 - val_accuracy: 0.8195\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.7913\n",
      "Epoch 00033: val_loss improved from 0.47092 to 0.46928, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 92s 580ms/step - loss: 0.5247 - accuracy: 0.7913 - val_loss: 0.4693 - val_accuracy: 0.8124\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5024 - accuracy: 0.8017\n",
      "Epoch 00034: val_loss did not improve from 0.46928\n",
      "159/159 [==============================] - 87s 549ms/step - loss: 0.5024 - accuracy: 0.8017 - val_loss: 0.4863 - val_accuracy: 0.8159\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.8031\n",
      "Epoch 00035: val_loss did not improve from 0.46928\n",
      "159/159 [==============================] - 87s 548ms/step - loss: 0.5012 - accuracy: 0.8031 - val_loss: 0.4808 - val_accuracy: 0.8283\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.8023\n",
      "Epoch 00036: val_loss improved from 0.46928 to 0.44749, saving model to ./model\\multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model\\multi_img_classification.model\\assets\n",
      "159/159 [==============================] - 91s 574ms/step - loss: 0.5079 - accuracy: 0.8023 - val_loss: 0.4475 - val_accuracy: 0.8212\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.7980\n",
      "Epoch 00037: val_loss did not improve from 0.44749\n",
      "159/159 [==============================] - 87s 547ms/step - loss: 0.5094 - accuracy: 0.7980 - val_loss: 0.4480 - val_accuracy: 0.8372\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.7986\n",
      "Epoch 00038: val_loss did not improve from 0.44749\n",
      "159/159 [==============================] - 87s 549ms/step - loss: 0.5075 - accuracy: 0.7986 - val_loss: 0.4895 - val_accuracy: 0.8018\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.8021\n",
      "Epoch 00039: val_loss did not improve from 0.44749\n",
      "159/159 [==============================] - 88s 556ms/step - loss: 0.4933 - accuracy: 0.8021 - val_loss: 0.4603 - val_accuracy: 0.8159\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.8063\n",
      "Epoch 00040: val_loss did not improve from 0.44749\n",
      "159/159 [==============================] - 87s 550ms/step - loss: 0.5060 - accuracy: 0.8063 - val_loss: 0.5085 - val_accuracy: 0.8053\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.8029\n",
      "Epoch 00041: val_loss did not improve from 0.44749\n",
      "159/159 [==============================] - 89s 559ms/step - loss: 0.4931 - accuracy: 0.8029 - val_loss: 0.4573 - val_accuracy: 0.8177\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 0.4891 - accuracy: 0.8112\n",
      "Epoch 00042: val_loss did not improve from 0.44749\n",
      "159/159 [==============================] - 87s 547ms/step - loss: 0.4891 - accuracy: 0.8112 - val_loss: 0.4634 - val_accuracy: 0.8248\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data.flow(x_train, y_train, batch_size=32), epochs=100, validation_data=(x_valid, y_valid), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1601053513314,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "6bLJIwKS7m2F",
    "outputId": "557fc091-b6b1-4d9b-8cf4-721fe475d722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 105ms/step - loss: 0.4623 - accuracy: 0.8284\n",
      "정확도 : 0.8284\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('modelLsleeve_haveCollar83.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1601053597096,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "IBxRSqcc7m2H",
    "outputId": "fa553552-b978-4ad0-b544-3db8c3da5676",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test data 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelNames = ['1_1', '1_2', '1_3', '1_4', '1_6', '1_7', '1_8', '1_9', '1_17']\n",
    "# labelNames = ['1_10', '1_11', '1_12']\n",
    "# labelNames = ['1_13', '1_14']\n",
    "classNames = ['jacket', 'coat', 'padding']\n",
    "\n",
    "dir = \"C:/Users/stell/Documents/graduation/image_sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1  파일 길이 :  450\n",
      "1_2  파일 길이 :  450\n",
      "1_3  파일 길이 :  450\n",
      "1_4  파일 길이 :  450\n",
      "1_6  파일 길이 :  450\n",
      "1_7  파일 길이 :  450\n",
      "1_8  파일 길이 :  450\n",
      "1_9  파일 길이 :  420\n",
      "1_17  파일 길이 :  450\n",
      "(4020, 125, 125, 3)\n",
      "(4020, 3)\n",
      "ok 4020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "fileNames = []\n",
    "\n",
    "for idx, cat in enumerate(labelNames):\n",
    "\n",
    "    label = [0 for i in range(len(classNames))]\n",
    "    label[0] = 1\n",
    "\n",
    "    imgDir = dir + \"/\" + cat\n",
    "    files = glob.glob(imgDir+\"/*.png\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "\n",
    "    for f in files:\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        data = np.asarray(img)\n",
    "            \n",
    "        x_test.append(data)\n",
    "        y_test.append(label)\n",
    "        fileNames.append(f)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(\"ok\", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"LHC_test\", x_test = x_test, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 125, 125, 3)\n"
     ]
    }
   ],
   "source": [
    "imageLoad = np.load(\"LHC_test.npz\")\n",
    "x_test = imageLoad['x_test']\n",
    "y_test = imageLoad['y_test']\n",
    "print(x_test.shape)\n",
    "\n",
    "x_train = x_train.astype(float) / 255\n",
    "x_test = x_test.astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2079,
     "status": "ok",
     "timestamp": 1601053605016,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "EI1st0RY_-HK",
    "outputId": "77d3a83a-55cd-4125-9407-310f3a51bf88"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1900e0b6a837>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'modelRawLHC82.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    180\u001b[0m     if (h5py is not None and (\n\u001b[0;32m    181\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 182\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    175\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[0;32m    178\u001b[0m                                                custom_objects=custom_objects)\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    169\u001b[0m   \"\"\"\n\u001b[0;32m    170\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[0;32m    172\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m'custom_objects'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         return cls.from_config(\n\u001b[0m\u001b[0;32m    355\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             custom_objects=dict(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0mimproperly\u001b[0m \u001b[0mformatted\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m     \"\"\"\n\u001b[1;32m--> 616\u001b[1;33m     input_tensors, output_tensors, created_layers = reconstruct_from_config(\n\u001b[0m\u001b[0;32m    617\u001b[0m         config, custom_objects)\n\u001b[0;32m    618\u001b[0m     model = cls(inputs=input_tensors, outputs=output_tensors,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mreconstruct_from_config\u001b[1;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[0;32m   1212\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1214\u001b[1;33m           \u001b[0mprocess_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m   \u001b[0minput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mprocess_node\u001b[1;34m(layer, node_data)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_tensors\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m       \u001b[0minput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnest_if_single_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1162\u001b[1;33m       \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m       \u001b[1;31m# Update node index map.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfused\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fused_batch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[1;31m# Currently never reaches here since fused_batch_norm does not support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    574\u001b[0m       \u001b[1;31m# pylint: enable=g-long-lambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m     output, mean, variance = tf_utils.smart_cond(training, train_op,\n\u001b[0m\u001b[0;32m    577\u001b[0m                                                  _fused_batch_norm_inference)\n\u001b[0;32m    578\u001b[0m     \u001b[0mvariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_add_or_remove_bessels_correction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     62\u001b[0m     return control_flow_ops.cond(\n\u001b[0;32m     63\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[1;32m---> 64\u001b[1;33m   return smart_module.smart_cond(\n\u001b[0m\u001b[0;32m     65\u001b[0m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                  name=name)\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[1;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[0;32m   1178\u001b[0m   if (util.EnableControlFlowV2(ops.get_default_graph()) and\n\u001b[0;32m   1179\u001b[0m       not context.executing_eagerly()):\n\u001b[1;32m-> 1180\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcond_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcond_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m   \u001b[1;31m# We needed to make true_fn/false_fn keyword arguments for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36mcond_v2\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mverify_captures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_COND\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrue_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfalse_graph\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     return _build_cond(\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mtrue_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_build_cond\u001b[1;34m(pred, true_graph, false_graph, true_inputs, false_inputs, building_gradient, name)\u001b[0m\n\u001b[0;32m    266\u001b[0m       \u001b[0mop_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_functional_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateless_if\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m     tensors = op_fn(\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mcond_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrue_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_functional_ops.py\u001b[0m in \u001b[0;36m_if\u001b[1;34m(cond, input, Tout, then_branch, else_branch, output_shapes, name)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \"'if' Op, not %r.\" % output_shapes)\n\u001b[0;32m    401\u001b[0m   \u001b[0moutput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_s\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[1;34m\"If\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthen_branch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthen_branch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m               else_branch=else_branch, output_shapes=output_shapes, name=name)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    407\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m           values = ops.internal_convert_n_to_tensor(\n\u001b[0m\u001b[0;32m    410\u001b[0m               \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m               \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_n_to_tensor\u001b[1;34m(values, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1559\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"%s_%d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1560\u001b[0m     ret.append(\n\u001b[1;32m-> 1561\u001b[1;33m         convert_to_tensor(\n\u001b[0m\u001b[0;32m   1562\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1563\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1465\u001b[0m         raise RuntimeError(\"Attempting to capture an EagerTensor without \"\n\u001b[0;32m   1466\u001b[0m                            \"building a function.\")\n\u001b[1;32m-> 1467\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mcapture\u001b[1;34m(self, tensor, name, shape)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[1;31m# Large EagerTensors and resources are captured with Placeholder ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capture_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_capture_helper\u001b[1;34m(self, tensor, name, shape)\u001b[0m\n\u001b[0;32m    645\u001b[0m     \u001b[0mcapture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_captures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcapture\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m       placeholder = _create_substitute_placeholder(\n\u001b[0m\u001b[0;32m    648\u001b[0m           tensor, name=name, dtype=tensor.dtype, shape=shape)\n\u001b[0;32m    649\u001b[0m       \u001b[1;31m# Record the composite device as an attribute to the placeholder.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_substitute_placeholder\u001b[1;34m(value, name, dtype, shape)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1130\u001b[1;33m     placeholder = graph_placeholder(\n\u001b[0m\u001b[0;32m   1131\u001b[0m         dtype=dtype or value.dtype, shape=shape, name=name)\n\u001b[0;32m   1132\u001b[0m   \u001b[0mcustom_gradient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\graph_only_ops.py\u001b[0m in \u001b[0;36mgraph_placeholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m     36\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m   \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdtype_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m   op = g._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m     39\u001b[0m       \u001b[1;34m\"Placeholder\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m       attrs=attrs, name=name)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         compute_device)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3475\u001b[0m     \u001b[1;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3476\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3477\u001b[1;33m       ret = Operation(\n\u001b[0m\u001b[0;32m   3478\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3479\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1972\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1974\u001b[1;33m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[0;32m   1975\u001b[0m                                 control_input_ops, op_def)\n\u001b[0;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('modelRawLHC82.h5')\n",
    "\n",
    "prediction = model.predict(x_test)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블\n",
    "    print(i)\n",
    "    print(\"해당 \"+fileNames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+classNames[pre_ans]+\"로 추정됩니다.\")\n",
    "    cnt += 1\n",
    "    print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
