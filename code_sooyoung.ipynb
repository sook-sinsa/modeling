{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3189,
     "status": "ok",
     "timestamp": 1601048600688,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "aIvnImlO7m1s",
    "outputId": "d8aae6e3-5791-4a66-ff74-362d7ecba2c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1601049242892,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "X7E2Jsjh7m1x"
   },
   "outputs": [],
   "source": [
    "# labelNames = ['nsleeve', 'ssleeve', 'lsleeve', 'jsuit', 'collar', 'hood', 'coat', 'jacket', 'lpadding', 'spadding', 'cardigan', 'vest', 'spants', 'lpants', 'skirt']\n",
    "# labelNames = ['0_0', '0_1', '0_2', '0_3_l', '0_3_s', '0_4_l', '0_4_s', '0_5', '0_6', '0_7',\n",
    "#              '1_0', '1_1', '1_2', '1_3', '1_4', '1_5', '1_6', '1_7', '1_8', '1_9', '1_10', '1_11', '1_12', '1_13', '1_14', '1_15', '1_16', '1_17',\n",
    "#              '2_0', '2_1', '2_2', '2_3',\n",
    "#              '3_0', '3_1', '3_2', '3_3', '3_4', '3_5',\n",
    "#              '4_0', '4_1', '4_2']\n",
    "# classMatching = [([0, 11], [1], [2, 5, 6, 7, 8, 9, 10], [12, 13], [14], [3], [4])]\n",
    "# classDic = { 0:1, 1:2, 2:0, 3:2, 4:1, 5:2, 6:1, 7:2, 8:2, 9:2,\n",
    "#            10:2, 11:2, 12:2, 13:2, 14:2, 15:2, 16:2, 17:2, 18:2, 19:2, 20:2, 21:2, 22:2, 23:2, 24:2, 25:0, 26:0, 27:2,\n",
    "#            28:5, 29:5, 30:5, 31:5,\n",
    "#            32:3, 33:3, 34:3, 35:3, 36:3, 37:3,\n",
    "#            38:4, 39:4, 40:4}\n",
    "\n",
    "# classNames = ['nsleeve', 'ssleeve', 'lsleeve', 'pants', 'skirt', 'dress']\n",
    "\n",
    "# classNames = ['nsleeve', 'vest']\n",
    "\n",
    "# classNames = ['ssleeve', 'collar']\n",
    "\n",
    "# classNames = ['sleeve', 'haveCollar', 'padding']\n",
    "# classNames = ['lsleeve', 'hood', 'cardigan']\n",
    "# classNames = ['coat', 'jacket', 'collar']\n",
    "# classNames = ['lpadding', 'spadding']\n",
    "\n",
    "# classNames = ['lpants', 'spants']\n",
    "\n",
    "# classNames = ['dress', 'jsuit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 numpy array로 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64633,
     "status": "ok",
     "timestamp": 1601049309058,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "dAxzAV-Q7m10",
    "outputId": "99a037ab-c539-4894-d8ca-ab12f73cace2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssleeve  파일 길이 :  450\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "lsleeve  파일 길이 :  450\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "nsleeve  파일 길이 :  450\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "collar  파일 길이 :  450\n",
      "[0, 0, 0, 0, 0, 1]\n",
      "hood  파일 길이 :  450\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "coat  파일 길이 :  450\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "jacket  파일 길이 :  450\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "lpadding  파일 길이 :  450\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "spadding  파일 길이 :  450\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "vest  파일 길이 :  450\n",
      "[1, 0, 0, 0, 0, 0]\n",
      "cardigan  파일 길이 :  450\n",
      "[0, 1, 0, 0, 0, 0]\n",
      "jsuit  파일 길이 :  423\n",
      "[0, 0, 0, 0, 1, 0]\n",
      "dress  파일 길이 :  450\n",
      "[0, 0, 0, 0, 1, 0]\n",
      "spants  파일 길이 :  450\n",
      "[0, 0, 1, 0, 0, 0]\n",
      "lpants  파일 길이 :  450\n",
      "[0, 0, 1, 0, 0, 0]\n",
      "skirt  파일 길이 :  450\n",
      "[0, 0, 0, 1, 0, 0]\n",
      "[[[[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]]\n",
      "\n",
      "\n",
      " [[[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]]\n",
      "\n",
      "\n",
      " [[[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]]\n",
      "\n",
      "\n",
      " [[[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]]\n",
      "\n",
      "\n",
      " [[[255 255 255]\n",
      "   [ 35  36  40]\n",
      "   [ 37  38  42]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [ 36  37  41]\n",
      "   [ 36  37  41]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [ 40  41  45]\n",
      "   [ 38  39  43]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]]]\n",
      "(6720, 125, 125, 3)\n",
      "(6720, 6)\n",
      "ok 6720\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "fileNames = []\n",
    "\n",
    "for idx, cat in enumerate(labelNames):\n",
    "\n",
    "    label = [0 for i in range(len(classNames))]\n",
    "    label[classDic[idx]] = 1\n",
    "\n",
    "    imgDir = dir + \"/\" + cat\n",
    "    files = glob.glob(imgDir+\"/*.png\")\n",
    "    print(cat, \" 파일 길이 : \", len(files))\n",
    "    for f in range(len(files)):\n",
    "        img = Image.open(files[f])\n",
    "        img = img.convert(\"RGB\")\n",
    "        data = np.asarray(img)\n",
    "        \n",
    "        if f >= 420:\n",
    "            fileNames.append(files[f])\n",
    "            x_test.append(data)\n",
    "            y_test.append(label)\n",
    "            \n",
    "        else :\n",
    "            x_train.append(data)\n",
    "            y_train.append(label)\n",
    "            \n",
    "        if f == 0:\n",
    "            print(label)\n",
    "\n",
    "# for idx in range(16) :\n",
    "#     imgDir = dir + '/' + labelNames[idx]\n",
    "#     files = glob.glob(imgDir+\"/*.png\")\n",
    "#     print(\"파일 길이 : \", len(files))\n",
    "    \n",
    "#     for f in files:\n",
    "#         img = Image.open(f)\n",
    "#         img = img.convert(\"RGB\")\n",
    "#         data = np.asarray(img, np.int32)\n",
    "        \n",
    "#         label = np.zeros(16)\n",
    "#         label[idx] = 1\n",
    "        \n",
    "#         x.append(data)\n",
    "#         y.append(label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"ok\", len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"imageData\", x_train = x_train, y_train = y_train, x_test = x_test, y_test = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 859,
     "status": "ok",
     "timestamp": 1601049350619,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "C2zpryl67m12",
    "outputId": "a687406b-5ef9-4613-ce97-d4de5cbb9add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6720, 125, 125, 3)\n",
      "(453, 125, 125, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2, os, glob, numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.config.experimental\n",
    "\n",
    "imageLoad = np.load(\"imageData.npz\")\n",
    "x_train = imageLoad['x_train']\n",
    "y_train = imageLoad['y_train']\n",
    "x_test = imageLoad['x_test']\n",
    "y_test = imageLoad['y_test']\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1601049564198,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "Kla7-Rb-7m15"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(float) / 255\n",
    "x_test = x_test.astype(float) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6048, 125, 125, 3)\n",
      "(672, 125, 125, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계 : CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1601052925682,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "5MUtohyz7m18"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=x_train.shape[1:], activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(classNames), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_dir = './model'\n",
    "    \n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1601052932510,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "X13t5r177m1_",
    "outputId": "c79dc477-96d0-49c2-a71a-98c79dbffe63",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 125, 125, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 321,574\n",
      "Trainable params: 321,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계 : VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", input_shape=x_train.shape[1:], activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3,3), padding=\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(classNames), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_dir = './model'\n",
    "    \n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "model_path = model_dir + '/multi_img_classification.model'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 140805,
     "status": "ok",
     "timestamp": 1601053428630,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "IF19f_if7m2C",
    "outputId": "e0f97556-a145-4a45-f5cd-436856413bb3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.5017 - accuracy: 0.4977\n",
      "Epoch 00001: val_loss improved from inf to 1.44213, saving model to ./model/multi_img_classification.model\n",
      "WARNING:tensorflow:From C:\\Users\\stell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 220s 1s/step - loss: 1.5017 - accuracy: 0.4977 - val_loss: 1.4421 - val_accuracy: 0.4911\n",
      "Epoch 2/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.4410 - accuracy: 0.5010\n",
      "Epoch 00002: val_loss improved from 1.44213 to 1.36729, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 219s 1s/step - loss: 1.4410 - accuracy: 0.5010 - val_loss: 1.3673 - val_accuracy: 0.4955\n",
      "Epoch 3/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.3851 - accuracy: 0.5227\n",
      "Epoch 00003: val_loss improved from 1.36729 to 1.32585, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 215s 1s/step - loss: 1.3851 - accuracy: 0.5227 - val_loss: 1.3259 - val_accuracy: 0.5327\n",
      "Epoch 4/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.3211 - accuracy: 0.5461\n",
      "Epoch 00004: val_loss improved from 1.32585 to 1.20743, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 212s 1s/step - loss: 1.3211 - accuracy: 0.5461 - val_loss: 1.2074 - val_accuracy: 0.5625\n",
      "Epoch 5/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.2492 - accuracy: 0.5615\n",
      "Epoch 00005: val_loss improved from 1.20743 to 1.10448, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 213s 1s/step - loss: 1.2492 - accuracy: 0.5615 - val_loss: 1.1045 - val_accuracy: 0.5967\n",
      "Epoch 6/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.2060 - accuracy: 0.5787\n",
      "Epoch 00006: val_loss improved from 1.10448 to 1.09426, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 214s 1s/step - loss: 1.2060 - accuracy: 0.5787 - val_loss: 1.0943 - val_accuracy: 0.5923\n",
      "Epoch 7/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.1597 - accuracy: 0.5971\n",
      "Epoch 00007: val_loss improved from 1.09426 to 0.98665, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 212s 1s/step - loss: 1.1597 - accuracy: 0.5971 - val_loss: 0.9866 - val_accuracy: 0.6295\n",
      "Epoch 8/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.1176 - accuracy: 0.6078\n",
      "Epoch 00008: val_loss did not improve from 0.98665\n",
      "189/189 [==============================] - 202s 1s/step - loss: 1.1176 - accuracy: 0.6078 - val_loss: 1.0424 - val_accuracy: 0.6339\n",
      "Epoch 9/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.0938 - accuracy: 0.6157\n",
      "Epoch 00009: val_loss improved from 0.98665 to 0.94388, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 215s 1s/step - loss: 1.0938 - accuracy: 0.6157 - val_loss: 0.9439 - val_accuracy: 0.6414\n",
      "Epoch 10/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.0650 - accuracy: 0.6255\n",
      "Epoch 00010: val_loss improved from 0.94388 to 0.88038, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 213s 1s/step - loss: 1.0650 - accuracy: 0.6255 - val_loss: 0.8804 - val_accuracy: 0.6860\n",
      "Epoch 11/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.0296 - accuracy: 0.6367\n",
      "Epoch 00011: val_loss did not improve from 0.88038\n",
      "189/189 [==============================] - 201s 1s/step - loss: 1.0296 - accuracy: 0.6367 - val_loss: 1.0034 - val_accuracy: 0.6295\n",
      "Epoch 12/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.0188 - accuracy: 0.6450\n",
      "Epoch 00012: val_loss improved from 0.88038 to 0.85362, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 215s 1s/step - loss: 1.0188 - accuracy: 0.6450 - val_loss: 0.8536 - val_accuracy: 0.6830\n",
      "Epoch 13/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.0012 - accuracy: 0.6445\n",
      "Epoch 00013: val_loss did not improve from 0.85362\n",
      "189/189 [==============================] - 199s 1s/step - loss: 1.0012 - accuracy: 0.6445 - val_loss: 0.8742 - val_accuracy: 0.6815\n",
      "Epoch 14/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.9615 - accuracy: 0.6639\n",
      "Epoch 00014: val_loss improved from 0.85362 to 0.82859, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 214s 1s/step - loss: 0.9615 - accuracy: 0.6639 - val_loss: 0.8286 - val_accuracy: 0.6905\n",
      "Epoch 15/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.9498 - accuracy: 0.6680\n",
      "Epoch 00015: val_loss improved from 0.82859 to 0.80269, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 209s 1s/step - loss: 0.9498 - accuracy: 0.6680 - val_loss: 0.8027 - val_accuracy: 0.7068\n",
      "Epoch 16/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.9228 - accuracy: 0.6753\n",
      "Epoch 00016: val_loss improved from 0.80269 to 0.78984, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 193s 1s/step - loss: 0.9228 - accuracy: 0.6753 - val_loss: 0.7898 - val_accuracy: 0.7009\n",
      "Epoch 17/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.6796\n",
      "Epoch 00017: val_loss improved from 0.78984 to 0.76844, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 215s 1s/step - loss: 0.9172 - accuracy: 0.6796 - val_loss: 0.7684 - val_accuracy: 0.7247\n",
      "Epoch 18/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.9120 - accuracy: 0.6819\n",
      "Epoch 00018: val_loss did not improve from 0.76844\n",
      "189/189 [==============================] - 204s 1s/step - loss: 0.9120 - accuracy: 0.6819 - val_loss: 0.7982 - val_accuracy: 0.7455\n",
      "Epoch 19/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.9272 - accuracy: 0.6756\n",
      "Epoch 00019: val_loss did not improve from 0.76844\n",
      "189/189 [==============================] - 202s 1s/step - loss: 0.9272 - accuracy: 0.6756 - val_loss: 0.7970 - val_accuracy: 0.7113\n",
      "Epoch 20/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8836 - accuracy: 0.6938\n",
      "Epoch 00020: val_loss improved from 0.76844 to 0.73120, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 217s 1s/step - loss: 0.8836 - accuracy: 0.6938 - val_loss: 0.7312 - val_accuracy: 0.7515\n",
      "Epoch 21/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8765 - accuracy: 0.6964\n",
      "Epoch 00021: val_loss did not improve from 0.73120\n",
      "189/189 [==============================] - 203s 1s/step - loss: 0.8765 - accuracy: 0.6964 - val_loss: 0.7520 - val_accuracy: 0.7307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8761 - accuracy: 0.6987\n",
      "Epoch 00022: val_loss did not improve from 0.73120\n",
      "189/189 [==============================] - 201s 1s/step - loss: 0.8761 - accuracy: 0.6987 - val_loss: 0.7419 - val_accuracy: 0.7411\n",
      "Epoch 23/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8653 - accuracy: 0.7049\n",
      "Epoch 00023: val_loss did not improve from 0.73120\n",
      "189/189 [==============================] - 200s 1s/step - loss: 0.8653 - accuracy: 0.7049 - val_loss: 0.7769 - val_accuracy: 0.7158\n",
      "Epoch 24/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8528 - accuracy: 0.7059\n",
      "Epoch 00024: val_loss improved from 0.73120 to 0.68629, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 216s 1s/step - loss: 0.8528 - accuracy: 0.7059 - val_loss: 0.6863 - val_accuracy: 0.7545\n",
      "Epoch 25/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8474 - accuracy: 0.7102\n",
      "Epoch 00025: val_loss did not improve from 0.68629\n",
      "189/189 [==============================] - 199s 1s/step - loss: 0.8474 - accuracy: 0.7102 - val_loss: 0.7158 - val_accuracy: 0.7485\n",
      "Epoch 26/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8493 - accuracy: 0.7030\n",
      "Epoch 00026: val_loss did not improve from 0.68629\n",
      "189/189 [==============================] - 203s 1s/step - loss: 0.8493 - accuracy: 0.7030 - val_loss: 0.7121 - val_accuracy: 0.7396\n",
      "Epoch 27/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8460 - accuracy: 0.7108\n",
      "Epoch 00027: val_loss did not improve from 0.68629\n",
      "189/189 [==============================] - 209s 1s/step - loss: 0.8460 - accuracy: 0.7108 - val_loss: 0.7338 - val_accuracy: 0.7366\n",
      "Epoch 28/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8229 - accuracy: 0.7173\n",
      "Epoch 00028: val_loss improved from 0.68629 to 0.66854, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 216s 1s/step - loss: 0.8229 - accuracy: 0.7173 - val_loss: 0.6685 - val_accuracy: 0.7515\n",
      "Epoch 29/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8446 - accuracy: 0.7014\n",
      "Epoch 00029: val_loss did not improve from 0.66854\n",
      "189/189 [==============================] - 200s 1s/step - loss: 0.8446 - accuracy: 0.7014 - val_loss: 0.6781 - val_accuracy: 0.7619\n",
      "Epoch 30/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8174 - accuracy: 0.7186\n",
      "Epoch 00030: val_loss did not improve from 0.66854\n",
      "189/189 [==============================] - 199s 1s/step - loss: 0.8174 - accuracy: 0.7186 - val_loss: 0.7194 - val_accuracy: 0.7396\n",
      "Epoch 31/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8143 - accuracy: 0.7140\n",
      "Epoch 00031: val_loss did not improve from 0.66854\n",
      "189/189 [==============================] - 206s 1s/step - loss: 0.8143 - accuracy: 0.7140 - val_loss: 0.6782 - val_accuracy: 0.7664\n",
      "Epoch 32/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7974 - accuracy: 0.7270\n",
      "Epoch 00032: val_loss did not improve from 0.66854\n",
      "189/189 [==============================] - 202s 1s/step - loss: 0.7974 - accuracy: 0.7270 - val_loss: 0.6700 - val_accuracy: 0.7574\n",
      "Epoch 33/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7978 - accuracy: 0.7273\n",
      "Epoch 00033: val_loss did not improve from 0.66854\n",
      "189/189 [==============================] - 203s 1s/step - loss: 0.7978 - accuracy: 0.7273 - val_loss: 0.6986 - val_accuracy: 0.7693\n",
      "Epoch 34/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.8044 - accuracy: 0.7242\n",
      "Epoch 00034: val_loss improved from 0.66854 to 0.65378, saving model to ./model/multi_img_classification.model\n",
      "INFO:tensorflow:Assets written to: ./model/multi_img_classification.model\\assets\n",
      "189/189 [==============================] - 130s 687ms/step - loss: 0.8044 - accuracy: 0.7242 - val_loss: 0.6538 - val_accuracy: 0.7679\n",
      "Epoch 35/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7984 - accuracy: 0.7272\n",
      "Epoch 00035: val_loss did not improve from 0.65378\n",
      "189/189 [==============================] - 104s 550ms/step - loss: 0.7984 - accuracy: 0.7272 - val_loss: 0.7032 - val_accuracy: 0.7470\n",
      "Epoch 36/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7795 - accuracy: 0.7330\n",
      "Epoch 00036: val_loss did not improve from 0.65378\n",
      "189/189 [==============================] - 112s 590ms/step - loss: 0.7795 - accuracy: 0.7330 - val_loss: 0.6684 - val_accuracy: 0.7753\n",
      "Epoch 37/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7855 - accuracy: 0.7234\n",
      "Epoch 00037: val_loss did not improve from 0.65378\n",
      "189/189 [==============================] - 117s 618ms/step - loss: 0.7855 - accuracy: 0.7234 - val_loss: 0.6816 - val_accuracy: 0.7470\n",
      "Epoch 38/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7750 - accuracy: 0.7393\n",
      "Epoch 00038: val_loss did not improve from 0.65378\n",
      "189/189 [==============================] - 115s 606ms/step - loss: 0.7750 - accuracy: 0.7393 - val_loss: 0.6684 - val_accuracy: 0.7589\n",
      "Epoch 39/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7804 - accuracy: 0.7242\n",
      "Epoch 00039: val_loss did not improve from 0.65378\n",
      "189/189 [==============================] - 122s 645ms/step - loss: 0.7804 - accuracy: 0.7242 - val_loss: 0.6731 - val_accuracy: 0.7515\n",
      "Epoch 40/100\n",
      "189/189 [==============================] - ETA: 0s - loss: 0.7810 - accuracy: 0.7323\n",
      "Epoch 00040: val_loss did not improve from 0.65378\n",
      "189/189 [==============================] - 118s 625ms/step - loss: 0.7810 - accuracy: 0.7323 - val_loss: 0.6660 - val_accuracy: 0.7634\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data.flow(x_train, y_train, batch_size=32), epochs=100, validation_data=(x_valid, y_valid), callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1601053513314,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "6bLJIwKS7m2F",
    "outputId": "557fc091-b6b1-4d9b-8cf4-721fe475d722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 2s 104ms/step - loss: 0.7690 - accuracy: 0.7152\n",
      "정확도 : 0.7152\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.4f\" % (model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1601053597096,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "IBxRSqcc7m2H",
    "outputId": "fa553552-b978-4ad0-b544-3db8c3da5676",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test data 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2079,
     "status": "ok",
     "timestamp": 1601053605016,
     "user": {
      "displayName": "SOOYOUNG LIM",
      "photoUrl": "",
      "userId": "09784489864364396866"
     },
     "user_tz": -540
    },
    "id": "EI1st0RY_-HK",
    "outputId": "77d3a83a-55cd-4125-9407-310f3a51bf88"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f57086852ba9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"{0:0.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "prediction = model.predict(x_test)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "cnt = 0\n",
    "\n",
    "for i in prediction:\n",
    "    pre_ans = i.argmax()  # 예측 레이블\n",
    "    print(i)\n",
    "    print(\"해당 \"+fileNames[cnt].split(\"\\\\\")[0]+\"이미지는 \"+classNames[pre_ans]+\"로 추정됩니다.\")\n",
    "    cnt += 1\n",
    "    print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
